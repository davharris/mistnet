% Generated by roxygen2 (4.0.1): do not edit by hand
\name{mistnet}
\alias{mistnet}
\title{Construct a mistnet model}
\usage{
mistnet(x, y, layer.definitions, loss, updater,
  sampler = gaussian.sampler(ncol = 10L, sd = 1), n.importance.samples = 25,
  n.minibatch = 25, training.iterations = 0, shuffle = TRUE,
  initialize.biases = TRUE, initialize.weights = TRUE)
}
\arguments{
\item{x}{a \code{numeric} \code{matrix} of predictor variables.  One row per
record, one column per predictive feature.}

\item{y}{a \code{matrix} of responses to \code{x}.  One row per record, one
column per response variable.}

\item{layer.definitions}{a \code{list} of specifications for each layer in
the network, as produced by \code{\link{defineLayer}}.}

\item{loss}{a \code{\link{loss}} object, defining the function for
optimization to minimize, as well as its gradient.}

\item{updater}{an \code{\link{updater}} object, specifying how the model
should move across the likelihood surface (e.g. stochastic gradient descent
or adagrad)}

\item{sampler}{a \code{\link{sampler}} object, specifying the distribution of
the latent variables}

\item{n.importance.samples}{an \code{integer}. More samples will take more
time to compute, but will provide a more precise estimate of the likelihood
gradient.}

\item{n.minibatch}{an \code{integer} specifying the number of rows to include
in each stochastic estimate of the likelihood gradient.}

\item{training.iterations}{an \code{integer} number of minibatches to process
before terminating. Defaults to zero so that the user can adjust the
network before training begins.}

\item{shuffle}{logical.  Should the data be shuffled after each epoch?
Defaults to TRUE.}

\item{initialize.biases}{logical.  Should the network's final layer's biases
be initialized to nonzero values? If \code{TRUE}, initial values will
depend on the \code{\link{nonlinearity}} of the final layer. Otherwise, all
values will be zero.}

\item{initialize.weights}{logical.  Should the weights in each layer be
initialized automatically? If \code{TRUE}, each \code{\link{layer}}'s
weights will be sampled randomly from their \code{\link{prior}}s.
Otherwise, all values will be zero, which can prevent the network from
learning.}
}
\description{
This function creates a \code{network} object for fitting a mistnet model.
}
\details{
The \code{mistnet} function produces a \code{\link{network}} object
  that produces a joint distribution over \code{y} given \code{x}. This
  distribution is defined by a stochastic feed-forward neural network (Neal
  1992), which is trained using a variant of backpropagation described in
  Tang and Salakhutdinov (2013) and Harris (2014). During each training
  iteration, model descends the gradient defined by its \code{\link{loss}}
  function, averaged over a number of Monte Carlo samples and a number of
  rows of data.

  A \code{\link{network}} concatenates the predictor variables in \code{x}
  with random variables produced by a \code{\link{sampler}} and passes the
  resulting data vectors through one or more \code{\link{layer}} objects to
  make predictions about \code{y}. The \code{weights} and \code{biases} in
  each \code{\link{layer}} can be trained using the \code{\link{network}}'s
  \code{fit} method (see example below).
}
\note{
\code{\link{network}} objects produced by \code{mistnet} are
  \code{\link{ReferenceClasses}}, and behave differently from other R
  objects. In particular, binding a \code{\link{network}} or other reference
  class object to a new variable name will not produce a copy of the original
  object, but will instead create a new alias for it.
}
\examples{
# 107 rows of fake data
x = matrix(rnorm(1819), nrow = 107, ncol = 17)
y = dropoutMask(107, 14)

# Create the network object
net = mistnet(
  x = x,
  y = y,
  layer.definitions = list(
    defineLayer(
      nonlinearity = rectify.nonlinearity(),
      size = 30,
      prior = gaussian.prior(mean = 0, sd = 0.1)
    ),
    defineLayer(
      nonlinearity = rectify.nonlinearity(),
      size = 12,
      prior = gaussian.prior(mean = 0, sd = 0.1)
    ),
    defineLayer(
      nonlinearity = sigmoid.nonlinearity(),
      size = ncol(y),
      prior = gaussian.prior(mean = 0, sd = 0.1)
    )
  ),
  loss = bernoulliLoss(),
  updater = adagrad.updater(learning.rate = .01),
  sampler = gaussian.sampler(ncol = 10L, sd = 1),
  n.importance.samples = 30,
  n.minibatch = 10,
  training.iterations = 0
)

# Fit the model
net$fit(iterations = 10)
}
\references{
Harris, D.J. Building realistic assemblages with a Joint Species
  Distribution Model. BioRxiv preprint. http://dx.doi.org/10.1101/003947

Neal, R.M. (1992) Connectionist learning of belief networks.
  Artificial Intelligence, 56, 71-113.

Tang, Y. & Salakhutdinov, R. (2013) Learning Stochastic
  Feedforward Neural Networks. Advances in Neural Information Processing
  Systems 26 (eds & trans C.J.C. Burges), L. Bottou), M. Welling), Z.
  Ghahramani), & K.Q. Weinberger), pp. 530-538.
}
\seealso{
\code{\link{network}}

\code{\link{layer}}
}

